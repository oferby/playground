{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\o00408152\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "global gama\n",
    "gama = 0\n",
    "states = 10\n",
    "actions = 2\n",
    "world = np.zeros(10)\n",
    "\n",
    "sess = tf.Session()\n",
    "x = tf.placeholder(tf.float32, shape=[None, states], name='input_states')\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, actions], name='correct_answer')\n",
    "W = tf.Variable(tf.zeros([10,2]), name='weights')\n",
    "b = tf.Variable(tf.zeros([2]), name='bias')\n",
    "\n",
    "# predicted action\n",
    "y = tf.nn.softmax(tf.matmul(x,W) + b)\n",
    "\n",
    "cross_antropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))\n",
    "train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_antropy)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_next_state(state, gama):\n",
    "    # act greedy with random\n",
    "    coin = np.random.randint(500 + gama)\n",
    "    if coin > 400:\n",
    "        a = get_best_action_from_nn(state)\n",
    "    else:\n",
    "        a = np.random.randint(actions)\n",
    "    s = get_next_valid_state(state, a)\n",
    "\n",
    "    return s, a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_next_valid_state(state, action):\n",
    "    if action == 0:\n",
    "        if state == 0:\n",
    "            return state\n",
    "        return state-1\n",
    "    if state == 9:\n",
    "        return state\n",
    "    return state+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_current_reward(state):\n",
    "    if state == 0:\n",
    "        return 0\n",
    "    if state == 9:\n",
    "        return 1\n",
    "    return 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_state_as_array(state):\n",
    "    s = np.zeros(10, dtype=np.float32)\n",
    "    s[state] = 1.0\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_reward_from_nn(state, a):\n",
    "    s = get_state_as_array(state)\n",
    "    b = sess.run(y, feed_dict={x: [s]})\n",
    "    return b[0,a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_best_action_from_nn(state):\n",
    "    s = get_state_as_array(state)\n",
    "    a = sess.run(tf.arg_max(y,1), feed_dict={x: [s]})\n",
    "    return a[0]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_max_reward_from_nn(state):\n",
    "    s = get_state_as_array(state)\n",
    "    m = sess.run(tf.reduce_max(y,1), feed_dict={x: [s]})\n",
    "    return m[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_all_rewards(state):\n",
    "    s = get_state_as_array(state)\n",
    "    b = sess.run(y, feed_dict={x: [s]})\n",
    "    return b[0].tolist()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train(state, rewards):\n",
    "    s = get_state_as_array(state)\n",
    "    sess.run(train_step, feed_dict={x:[s], y_:[rewards]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "state = np.random.randint(1, states-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-8-8bdd353f2a3b>:3: arg_max (from tensorflow.python.ops.gen_math_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse `argmax` instead\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    r= []\n",
    "    nn = []\n",
    "    \n",
    "    # update value function\n",
    "    for action in range(actions):\n",
    "        ns = get_next_valid_state(state, action)\n",
    "        nn.append(ns)\n",
    "        r.append(get_max_reward_from_nn(ns))\n",
    "    pr = get_all_rewards(state)\n",
    "    \n",
    "    # update NN\n",
    "    if not r == pr:\n",
    "        train(state, r)\n",
    "    \n",
    "    # act greedy\n",
    "    \n",
    "    ls = state\n",
    "    state, la = get_next_state(state, gama)\n",
    "    \n",
    "    # get current state value\n",
    "    r = get_current_reward(state)\n",
    "    \n",
    "    # update privious state and action\n",
    "    if r != 0.5:\n",
    "        rewards = get_all_rewards(ls)\n",
    "        rewards[la] = r\n",
    "        train(ls, rewards)\n",
    "            \n",
    "    gama+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.44866493344306946, 0.5513350367546082],\n",
       " [0.44866493344306946, 0.5513350367546082],\n",
       " [0.44866493344306946, 0.5513350367546082],\n",
       " [0.44866493344306946, 0.5513350367546082],\n",
       " [0.44866493344306946, 0.5513350367546082],\n",
       " [0.45134997367858887, 0.5486499667167664],\n",
       " [0.4525884985923767, 0.5474115014076233],\n",
       " [0.4388071596622467, 0.5611928701400757],\n",
       " [0.35374459624290466, 0.646255373954773],\n",
       " [0.4994153678417206, 0.500584602355957]]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yy = []\n",
    "for i in range(10):\n",
    "    yy.append(get_all_rewards(i))\n",
    "yy\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
