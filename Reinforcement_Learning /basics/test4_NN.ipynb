{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "global gama\n",
    "states = 100\n",
    "actions = 4\n",
    "reward = -0.01\n",
    "gama = 0\n",
    "world = np.zeros((states, actions))\n",
    "\n",
    "pits = []\n",
    "j = 0\n",
    "while True:\n",
    "    p = np.random.randint(1,99)\n",
    "    if p not in pits:\n",
    "        pits.append(p)\n",
    "        j+=1\n",
    "        if j > 10:\n",
    "            break\n",
    "\n",
    "wins = [0,99]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "x = tf.placeholder(tf.float32, shape=[None, states], name='input_states')\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, actions], name='correct_answer')\n",
    "W = tf.Variable(tf.zeros([states ,actions]), name='weights')\n",
    "b = tf.Variable(tf.zeros([actions]), name='bias')\n",
    "\n",
    "# predicted action\n",
    "y = tf.matmul(x,W) + b\n",
    "y_soft = tf.nn.softmax(y)\n",
    "\n",
    "cross_antropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))\n",
    "train_step = tf.train.GradientDescentOptimizer(0.1).minimize(cross_antropy)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_next_state(state, gama):\n",
    "    # act greedy with random\n",
    "    coin = np.random.randint(500 + gama)\n",
    "    a = get_best_action_from_nn(state)\n",
    "    if coin < 400:\n",
    "        random_actions = []\n",
    "        if x in actions:\n",
    "            if x !=a:\n",
    "                random_actions.append(x)\n",
    "        a = np.random.randint(random_actions)\n",
    "    s = get_next_valid_state(state, a)\n",
    "\n",
    "    return s, a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LEFT = 0\n",
    "RIGHT = 1\n",
    "DOWN = 2\n",
    "UP = 3\n",
    "\n",
    "def get_possible_actions(state):\n",
    "    x = int(state % np.sqrt(states))\n",
    "    y = int(state // np.sqrt(states))\n",
    "    actions=[]\n",
    "    if x > 0:\n",
    "        actions.append(LEFT)\n",
    "        if x < (np.sqrt(states) - 1):\n",
    "            actions.append(RIGHT)        \n",
    "    elif x < (np.sqrt(states) - 1):\n",
    "            actions.append(RIGHT)\n",
    "\n",
    "    if y > 0:\n",
    "        actions.append(DOWN)\n",
    "        if y < (np.sqrt(states) - 1):\n",
    "            actions.append(UP)\n",
    "    elif y < (np.sqrt(states) - 1):\n",
    "            actions.append(UP)\n",
    "    return actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_next_valid_state(state, action):\n",
    "    x = int(state % np.sqrt(states))\n",
    "    y = int(state // np.sqrt(states))   \n",
    "    if x > 0 and action == LEFT:\n",
    "        return state - 1\n",
    "    if x < 9 and action == RIGHT:\n",
    "        return state + 1\n",
    "    if y > 0 and action == DOWN:\n",
    "        return state - int(np.sqrt(states)) \n",
    "    if y < 9 and action == UP:\n",
    "        return state + int(np.sqrt(states))\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_current_reward(state):\n",
    "    if state in wins:\n",
    "        return 1\n",
    "    if state in pits:\n",
    "        return 0\n",
    "    return 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_state_as_array(state):\n",
    "    s = np.zeros(states, dtype=np.float32)\n",
    "    s[state] = 1.0\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_reward_from_nn(state, a):\n",
    "    s = get_state_as_array(state)\n",
    "    b = sess.run(y, feed_dict={x: [s]})\n",
    "    return b[0,a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_best_action_from_nn(state):\n",
    "    s = get_state_as_array(state)\n",
    "    a = sess.run(tf.arg_max(y,1), feed_dict={x: [s]})\n",
    "    return a[0]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_max_reward_from_nn(state):\n",
    "    s = get_state_as_array(state)\n",
    "    m = sess.run(tf.reduce_max(y,1), feed_dict={x: [s]})\n",
    "    return m[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_all_rewards(state):\n",
    "    s = get_state_as_array(state)\n",
    "    b = sess.run(y, feed_dict={x: [s]})\n",
    "    return b[0].tolist()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def train(state, rewards):\n",
    "    s = get_state_as_array(state)\n",
    "    for i in range(5):\n",
    "        sess.run(train_step, feed_dict={x:[s], y_:[rewards]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_first_state():\n",
    "    found = False\n",
    "    while not found:\n",
    "        state = np.random.randint(states)\n",
    "        if state not in wins and state not in pits:\n",
    "            return state\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def update_rewards(state):\n",
    "    possible_actions = get_possible_actions(state)\n",
    "    all_rewards = get_all_rewards(state)\n",
    "\n",
    "    for action in range(actions):\n",
    "        if action in possible_actions:\n",
    "            next_state = get_next_valid_state(state, action)\n",
    "            reward = get_max_reward_from_nn(next_state)\n",
    "            all_rewards[action] = reward - 0.1\n",
    "        else:\n",
    "            all_rewards[action] = 0\n",
    "#     all_rewards = softmax(all_rewards)\n",
    "    return all_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "state = get_first_state()\n",
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for i in range(20):\n",
    "    \n",
    "    # update NN\n",
    "    all_rewards = update_rewards(state)\n",
    "    train(state, all_rewards)\n",
    "    \n",
    "    ls = state\n",
    "    # Act greedy\n",
    "    state, la = get_next_state(state, gama)\n",
    "\n",
    "    # get current state value\n",
    "    r = get_current_reward(state)\n",
    "\n",
    "    # update privious state and action\n",
    "    rewards = update_rewards(ls)\n",
    "\n",
    "    if r != 0.5:\n",
    "        rewards[la] = r\n",
    "\n",
    "#     rewards = softmax(rewards)\n",
    "\n",
    "    # update privious state\n",
    "    train(ls, rewards)\n",
    "    gama+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "board = [\"\" for i in range(100)]\n",
    "\n",
    "for i in range(states):\n",
    "        a = get_best_action_from_nn(i)\n",
    "        if a == LEFT:\n",
    "            board[i] = '<'\n",
    "        elif a == RIGHT:\n",
    "            board[i] = '>'\n",
    "        elif a == DOWN:\n",
    "            board[i] = '^'\n",
    "        else:\n",
    "            board[i] = 'V'\n",
    "\n",
    "for p in pits:\n",
    "    board[p] = 'O'\n",
    "board[99] = 'X'\n",
    "board[0] = 'X'\n",
    "board = np.reshape(board, (10,10))\n",
    "\n",
    "\n",
    "board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ls = 24\n",
    "# la = LEFT\n",
    "\n",
    "# state = 23\n",
    "\n",
    "for k in range(2000):\n",
    "\n",
    "\n",
    "    cr = get_current_reward(state)\n",
    "\n",
    "    pa = get_possible_actions(ls)\n",
    "    rr = []\n",
    "    for i in range(actions):\n",
    "        if i in pa:\n",
    "            s1 = get_next_valid_state(ls, i)\n",
    "            r1 = get_max_reward_from_nn(s1)\n",
    "            rr.append(r1 - 0.1)\n",
    "        else:\n",
    "            rr.append(0)\n",
    "    if cr !=0.5:\n",
    "        rr[la] = cr\n",
    "\n",
    "    # print ('%s: %s\\n' % (ls,rr))\n",
    "\n",
    "    s = get_state_as_array(ls)\n",
    "    for i in range(50):\n",
    "        sess.run(train_step, feed_dict={x:[s], y_:[rr]})\n",
    "\n",
    "    ls = state\n",
    "    while True:\n",
    "        state, la = get_next_state(state, 100)\n",
    "        if state != ls:\n",
    "            break\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ww = sess.run(W, feed_dict={x:[s], y_:[rr]})\n",
    "bb = sess.run(b, feed_dict={x:[s], y_:[rr]})\n",
    "# ww, bb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# update NN\n",
    "all_rewards = update_rewards(state)\n",
    "train(state, all_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ls = state\n",
    "# Act greedy\n",
    "state, la = get_next_state(state, gama)\n",
    "ls, la, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get current state value\n",
    "r = get_current_reward(state)\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# update privious state and action\n",
    "rewards = update_rewards(ls)\n",
    "\n",
    "if r != 0.5:\n",
    "    rewards[la] = r\n",
    "\n",
    "#     rewards = softmax(rewards)\n",
    "\n",
    "# update privious state\n",
    "train(ls, rewards)\n",
    "gama+=1\n",
    "ls, la, rewards"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
